\documentclass[]{marticle}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{amssymb}
\usepackage{mstyle}

\title{\textbf{\Huge Relazione di Laboratorio Computazionale}}
\date{}


\begin{document}
\maketitle

\textbf{Le parti in grassetto sono commenti generici o placeholder perch\`e
QUESTA \`E UNA BOZZA!}

\section*{Abstract}
In questa relazione prenderemo in considerazione il problema di estrarre
campioni di valori casuali data una certa distribuzione di probabilita` discreta.
Supporremo di sapere generare variabili uniformi sull'intervallo $[0,1]$, che
verranno implementate operativamente come le variabili generate dalla libreria
\code{numpy}.

Una prima soluzione del problema \`e quella di dividere $[0,1]$ in intervalli di
lunghezza pari a ciascuna componente del vettore di probabilit\`a, e scegliere
il risultato in funzione dell'intervallo a cui appartiene una variabile
uniforme. Questo presenta diversi inconvenienti: il metodo infatti richiede un
numero di somme proporzionale al numero di componenti del vettore di
probabilit\`a. Questo potrebbe essere intrattabile quando molto grande. Inoltre
spesso il vettore di probabilit\`a \`e noto solo a meno di un coefficiente di
normalizzazione, il cui calcolo richiederebbe di nuovo $O(n)$ somme. 

Uno dei metodi pi\`u utilizzati per ovviare a questi problemi \`e il metodo di
Monte Carlo (abbreviato spesso con MCMC, Markov Chain Monte Carlo), che consiste
nella simulazione di una camminata su una catena di Markov con distribuzione
invariante la distribuzione data. Dopo un numero sufficiente di step, la
frequenza di visita di un nodo sar\`a arbitrariamente vicina a quella voluta. In
questo caso per\`o il numero di passi necessari ad una determinata distribuzione
non \`e noto a priori ed \`e di difficile calcolo.

Si andr\`a dunque a presentare l'algoritmo di Propp-Wilson, una modifica del
MCMC che ha il vantaggio di ottenere la distribuzione esatta e di terminare una
volta raggiunta questa. Applicheremo tale algoritmo al modello di Ising, una
modellizzazione del comportamento magnetico della materia.

\section{Il modello di Ising}
Sia $G=(V,E)$ un grafo. I vertici andranno a rappresentare i singoli atomi di un
materiale, e gli archi indicano quali atomi interagiscono fra loro. Ad ogni
atomo viene quindi associato uno spin che pu\`o essere $+1$ o $-1$. Una
configurazione \`e quindi una funzione $f\colon V \rightarrow \{+1, -1\}$. A
ciascuno di questi modelli si associa l'energia 
\[
    H(f) = \sum_{(x,y)\in E} f(x)f(y)
\].
Inoltre viene dato un parametro reale del sistema $\beta \geq 0$ detta
temperatura inversa. Il modello di ising associa ad ogni configurazione $f \in
\{+1, -1\}^V$ la probabilit\`a
\[
    \pi(f) = \frac{1}{Z} \exp(-\beta H(f))
\]
Dove $Z$ \`e il coefficiente di normalizzazione pari a
\[
    Z = \sum_{f \in \{+1,-1\}^V} \exp(-\beta H(f))
\]

L'obiettivo che ci prefiggiamo \`e quello di estrarre un campione da
$\{+1,-1\}^V$ con probabilit\`a $\pi$.

\section{Metodo di Monte Carlo}

\textbf{Non so se ha davvero senso questa parte}

Come prima cosa, data una distribuzione $\pi$ su un insieme $\Omega$, cerchiamo
una matrice di transizione $P$ per una catena di Markov omogenea che abbia
$\pi$ come misura invariante. Per farlo chiediamo una condizione pi\`u forte su
$P$, cio\`e la reversibilit\`a, ovvero si vuole che $\pi (i)P_{i,j}=\pi
(j)P_{j,i}$. Se la matrice $P$ \`e irriducibile e aperiodica, data una
distribuzione iniziale $\pi_0$, la distribuzione di probabilit\`a dopo $n$ passi
$\pi_n = \pi_0 P^n$ converge a $\pi$

Cerchiamo allora le matrici del tipo $P_{i,j} = A_{i,j} Q_{i,j}$ per $i\neq j$,
dove la matrice $Q$ \`e irriducibile e viene detta matrice generatrice dei
candidati, e $A$ \`e da terminare in modo tale che $P$ abbia le propriet\`a
richieste e $0\leq A \leq 1$. Le componenti $P_{i,i}$ sono determinate in
maniera tale che $P$ sia stocastica.  A livello di interpretazione si pu\`o
pensare che a ogni step si sceglie vertice candidato a cui passare con
probabilit\`a dettata da $Q$ e poi si esegue il passagio con probabilit\`a
dettata da $A$, altrimenti si rimane nel vertice di partenza. Per questo motivo
$A$ \`e la matrice delle probabilit\`a di accettazione. 

Generalmente si sceglie $A$ della forma
\[
    A_{i,j} = \frac{S_{i,j}}{1+T_{i,j}}
\]
con
\[
    T_{i, j} = \frac{\pi{i} Q_{i,j}}{\pi{j} Q_{j,i}}
\]
e $S$ una matrice simmetrica.
L'algoritmo di Metropoli-Hastings sceglie
\[
    A_{i,j} = \min (1, \frac{\pi(i)}{\pi(j)}).
\]
 In questo modo si determina una $P$ con tutte le propriet\`a richieste.

\section{Il Gibbs sampler}

\textbf{come gestico le traduzioni?}
Vogliamo specializzare il MCMC ai casi in cui l'insime degli stati sia un
insieme di funzioni (con dominio e codominio finiti). Questa capita nel modello
di Ising che studieremo in seguito, per esempio. Abbiamo dunque un processo
stocastico a valori in $E=\Lambda^V$ per certi insiemi $\Lambda$ e $V$. 

Con il Gibbs sampler il processo passa da uno stato $X_n=f$ a $X_{n+1}=g$ con le
seguenti regole: sia $v$ un elemento scelto in modo casuale uniforme da $V$.
Allora $f$ e $g$ devono coincidere su $V\setminus\setof{v}$. Il punto $g(v)$
assume il valore $\lambda$ con probabilit\`a $\prob{X(v)=\lambda \ |\ 
X(V\setminus\setof{v})=f(V\setminus\setof{v})}$. \textbf{Si verifica che} la
distribuzione cercata soddisfa la condizione di reversibilit\`a e che la catena
risultante \`e irriducibile e aperiodica. Dunque si ha la converganza del metodo
di Monte Carlo.


\section{Coupling from the past}

In questa sezione svilupperemo un metodo per estrarre un campione con una
probabilit\`a esatta $\pi$, data $P$ una matrice di transizione irriducibile e
aperiodica Su un insieme finito di stati $S=\setof{s_1,\dots,s_n}$ e con
probabilit\`a invariante $\pi$. Possiamo associare alla catena di Markov
definita da $P$ una funzione di transizione 
\[
    f\colon S\times [0,1] \longrightarrow S
\]
tale che se $U$ \`e una variabile uniforme sull'intervallo $[0,1]$, allora 
\[
    \prob{f(s_i,U)=s_j}=P_{i,j} \qquad \forall s_i,s_j \in S.
\]
Questo pu\`o essere fatto, per esempio \textbf{nel modo ovvio}.

Siano $U_n$ variabili uniformi su $[0,1]$ per ogni $n\in \Z$. Costruiamo allora
delle sequenze $X^r_m(i)$ con $i = 1,\dots,n$, $r\leq 0$ e $m \geq r$ interi nel
modo seguente:
\[
    X^r_r(i) = s_i
\]
\[
    X^r_{m+1}(i) = f(X^r_m(i), U_m).
\]

Sia ora 
\[
    \tau^-=\max(\ r \st X^r_0(1) = \dots = X^r_0(n))
\]
e definiamo $Y = X^{\tau^-}_0(1)$. La condizione con cui \`e stato definito $\tau^-$
\`e detta coalescenza.

Si verifica che $Y$ ha distribuzione pari a $\pi$.

Questo metodo \`e noto come algoritmo di Propp-Wilson.

\textbf{Probabilmente questo ci va.}

\textbf{Probabilmente ci vanno anche dei controesempi per dire che il coupling
in the future non funziona e che gli $U$ devono essere sempre gli stessi, c'\`e
su Haggstrom pp.81-82.}

\textbf{Mettere i disegnini che sono strabelli?}
\section{Sandwiching}

Il metodo presentato sopra \`e funzionante e risolve gli scopi che ci eravamo
prefissi, tuttavia simulare tutte le $n$ catene di Markov \`e praticamente
impossibile per $n$ grandi. In questa sezione ci occuperemo di migliorare il
metodo per renderlo computazionalmente pi\`u leggero.
 
In questo caso supporremo di avere un ordine parziale tra gli stati in $S$ che
denoteremo con il simbolo $\preceq$. Richiediamo che la funzione di transizione
rispetti la condizione di monotonia
\[
    f(s_i, u) \leq f(s_j, u) \qquad \forall s_i \preceq s_j
\]
e che esistano in $S$ un massimo e un minimo, che senza perdita di generalit\`a
chiameremo $s_1$ e $s_n$. Si verifica facilmente che la coalescenza di $X^r_0$
si verifica se e solo se $X^r_0(1) = X^r_0(n)$. Inoltre il tempo di coalescenza
$\tau^-$ \`e quasi certamente finito. 

\textbf{Forse anche questa cosa si dovrebbe fare...}

\section{Implementazione del modello di Ising}

Andiamo ad utilizzare le tecnice viste al modello di Ising. Innanzi tutto ci
ridurremo al caso in cui il grafo \`e una griglia quadrata di lato $l$. Sui
possibili stati di un grafo, consideriamo la relazione d'ordine definita da:
$f \preceq g$ se e solo se per ogni vertice $v$ si ha che $f(v) \leq g(v)$.
Con questa realzione si hanno due stati particolari, le due funzioni costanti,
che sono il massimo e il minimo. Lo scopo \`e quello di simulare le catene di
Markov che partono da questo stato. 

Per farlo prenderemo in considerazione il modello di Gibbs. Ad ogni step di
configurazione $f$, si sceglie in modo casuale uniforme un vertice $v$, e lo si
pone a $+1$ con probabilit\`a
\[
    \frac{2\beta\sum_{(v,v')\in E} f(v')} {2\beta\sum_{(v,v')\in E}
    f(v')+1}.
\]
Lo si pone $-1$ altrimenti. Questa catena di Markov \`e monotona, infatti i
vertici che venogono aggiornati sono gli stessi per la catena che parte dal
massimo e quella che parte dal minimo, inoltre vengono aggiornati nello stesso
modo. \textbf{Per una cosa vista perlando del Gibbs sampler}, la distribuzione
stazionaria \`e quella del mmodello di Ising. 

\textbf{Viene fornito di seguito il codice per l'implementazione.}


\end{document}
